{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Neural Network - Corporación Favorita Grocery Sales Forecasting\n",
    "\n",
    "Brick-and-mortar grocery stores are always in a delicate dance with purchasing and sales forecasting. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leaving money on the table and customers fuming.\n",
    "\n",
    "The problem becomes more complex as retailers add new locations with unique needs, new products, ever transitioning seasonal tastes, and unpredictable product marketing. Corporación Favorita, a large Ecuadorian-based grocery retailer, knows this all too well. They operate hundreds of supermarkets, with over 200,000 different products on their shelves.\n",
    "\n",
    "Corporación Favorita has challenged the Kaggle community to build a model that more accurately forecasts product sales. They currently rely on subjective forecasting methods with very little data to back them up and very little automation to execute plans. They’re excited to see how machine learning could better ensure they please customers by having just enough of the right products at the right time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from neural_network import neural_net_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing_data import train_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train size\n",
    "train_size = 0.6\n",
    "\n",
    "# Split the train data to learn # Warning date -> split linearly\n",
    "X_train = train_all.drop(['unit_sales'],\n",
    "                         axis=1).loc[range(int(train_all.shape[0]*train_size))]\n",
    "y_train = train_all['unit_sales'].loc[range(int(train_all.shape[0]*train_size))]\n",
    "X_test = train_all.drop(['unit_sales'],\n",
    "                        axis=1).loc[range(int(train_all.shape[0]*train_size),\n",
    "                                    int(train_all.shape[0]))]\n",
    "y_test = train_all['unit_sales'].loc[range(int(train_all.shape[0]*train_size),\n",
    "                                     int(train_all.shape[0]))]\n",
    "\n",
    "# Norlmalize data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train.values)\n",
    "y_train_norm = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "X_test_norm = scaler.fit_transform(X_test.values)\n",
    "y_test_norm = scaler.fit_transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(np.max(X_train_norm), np.max(y_train_norm), np.min(y_train_norm),\n",
    "      np.min(y_train_norm))\n",
    "\n",
    "\n",
    "def denormalize(y_train, norm_data):\n",
    "    try:\n",
    "        df = y_train.values.reshape(-1, 1)\n",
    "    except AttributeError:\n",
    "        df = y_train.reshape(-1, 1)\n",
    "    norm_data = norm_data.reshape(-1, 1)\n",
    "    scl = MinMaxScaler()\n",
    "    scl.fit_transform(df)\n",
    "    return scl.inverse_transform(norm_data)\n",
    "\n",
    "\n",
    "# Input data\n",
    "nb_input = X_train.shape[1]\n",
    "nb_hidden1 = 128\n",
    "nb_hidden2 = 128\n",
    "batch_size = 100000\n",
    "nb_epoch = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III - Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "X_tf = tf.placeholder(tf.float32)\n",
    "y_tf = tf.placeholder(tf.float32)\n",
    "keep_prob_1 = tf.placeholder(tf.float32)\n",
    "keep_prob_2 = tf.placeholder(tf.float32)\n",
    "\n",
    "output, W_O = neural_net_model(X_tf, nb_input, nb_hidden1, nb_hidden2,\n",
    "                               keep_prob_1, keep_prob_2)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(output-y_tf))\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "correct_pred = tf.argmax(output, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "c_t = []\n",
    "c_test = []\n",
    "err_t = []  # norm l2\n",
    "err_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    y_t = denormalize(y_train, y_train_norm)\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    plt.xlabel('item')\n",
    "    plt.ylabel('number of units sales')\n",
    "    plt.title('Evolution of units sales - Grocery store')\n",
    "    ax.plot(range(len(y_train)), y_t, label='Original')\n",
    "    plt.show()\n",
    "\n",
    "    try:\n",
    "        saver.restore(sess, 'NN_favorita_grocery_sales.ckpt')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for i in tqdm(range(nb_epoch)):\n",
    "\n",
    "        # Define and create batch samples\n",
    "        batch_start = rd.randint(0, X_train_norm.shape[0]-batch_size)\n",
    "\n",
    "        X_train_norm_batch = X_train_norm[np.arange(batch_start,\n",
    "                                                    batch_start+batch_size),\n",
    "                                          :]\n",
    "        X_train_batch = X_train.loc[np.arange(batch_start,\n",
    "                                              batch_start+batch_size)]\n",
    "\n",
    "        y_train_norm_batch = y_train_norm[np.arange(batch_start,\n",
    "                                                    batch_start+batch_size)].reshape(-1, 1)\n",
    "        y_train_batch = y_train.loc[np.arange(batch_start,\n",
    "                                              batch_start+batch_size)]\n",
    "        y_train_batch = np.array(y_train_batch).reshape(-1, 1)\n",
    "\n",
    "        # Run training on batch\n",
    "        for j in range(X_train_norm_batch.shape[0]):\n",
    "            sess.run([cost, train],\n",
    "                     feed_dict={X_tf: X_train_norm_batch[j, :].reshape(1, nb_input),\n",
    "                                y_tf: y_train_norm_batch[j],\n",
    "                                keep_prob_1: 0.3,\n",
    "                                keep_prob_2: 0.3})\n",
    "        pred = sess.run(output, feed_dict={X_tf: X_train_norm_batch,\n",
    "                                           keep_prob_1: 1.0,\n",
    "                                           keep_prob_2: 1.0})\n",
    "        pred = denormalize(y_train_batch, pred)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        err_t.append(np.linalg.norm(pred - y_train_batch))\n",
    "        c_t.append(sess.run(cost, feed_dict={X_tf: X_train_norm_batch,\n",
    "                                             y_tf: y_train_norm_batch,\n",
    "                                             keep_prob_1: 1.0,\n",
    "                                             keep_prob_2: 1.0}))\n",
    "        c_test.append(sess.run(cost, feed_dict={X_tf: X_test_norm,\n",
    "                                                y_tf: y_test_norm,\n",
    "                                                keep_prob_1: 1.0,\n",
    "                                                keep_prob_2: 1.0}))\n",
    "        print('Epoch :', i, 'Cost :', c_t[i], 'Err (l2) :', err_t[i])\n",
    "\n",
    "    pred = sess.run(output, feed_dict={X_tf: X_test_norm,\n",
    "                                       keep_prob_1: 1.0,\n",
    "                                       keep_prob_2: 1.0})\n",
    "\n",
    "    print('Cost :', sess.run(cost, feed_dict={X_tf: X_test_norm,\n",
    "                                              y_tf: y_test_norm,\n",
    "                                              keep_prob_1: 1.0,\n",
    "                                              keep_prob_2: 1.0}))\n",
    "    y_test = denormalize(y_test, y_test_norm)\n",
    "    pred = denormalize(y_test, pred)\n",
    "\n",
    "    # Plot the accuracy as l2 norm\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(nb_epoch), err_t, label=\"err (l2 norm)\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel('l2 norm - error of the prediction')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.title('Evolution of the error of the prediction through epochs')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the prediction vs the original\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(y_test.shape[0]), y_test, label=\"Original Data\")\n",
    "    plt.plot(range(y_test.shape[0]), pred, label=\"Predicted Data\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel('units sales')\n",
    "    plt.xlabel('Days')\n",
    "    plt.title('Evolution of units sales - Grocery store')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the model\n",
    "    if input('Save model ? [Y/N]') == 'Y':\n",
    "        import os\n",
    "        saver.save(sess, os.getcwd() +\n",
    "                   '/nn_saved_sessions/NN_favorita_grocery_sales.ckpt')\n",
    "        print('Model Saved')\n",
    "\n",
    "    # Close the session\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V - Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy as l2 norm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(nb_epoch), err_t, label=\"err (l2 norm)\")\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('l2 norm - error of the prediction')\n",
    "plt.xlabel('epochs')\n",
    "plt.title('Evolution of the error of the prediction through epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prediction vs the original\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(y_test.shape[0]), y_test, label=\"Original Data\")\n",
    "plt.plot(range(y_test.shape[0]), pred, label=\"Predicted Data\")\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('units sales')\n",
    "plt.xlabel('Days')\n",
    "plt.title('Evolution of units sales - Grocery store')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
